---
---

@article{lorang2020digital,
  title={Digital libraries, intelligent data analytics, and augmented description: A demonstration project},
  author={Lorang, Elizabeth and Soh, Leen-Kiat and Liu, Yi and Pack, Chulwoo},
  year={2020},

  abbr={Tech. Report},
  blog={https://blogs.loc.gov/thesignal/2019/09/summer-of-machine-learning-collaboration-with-the-university-of-nebraska-lincoln/},
  html={https://digitalcommons.unl.edu/libraryscience/396/}
}

@inproceedings{pack2015computer,
  title={Computer aided breast cancer diagnosis system with fuzzy multiple-parameter support vector machine},
  author={Pack, Chulwoo and Shin, Sung and Son, Seong Ho and Jeon, Soon Ik},
  booktitle={Proceedings of the 2015 Conference on research in adaptive and convergent systems},
  pages={172--176},
  year={2015},

  abbr={RACS},
  html={https://dl.acm.org/doi/abs/10.1145/2811411.2811504}
}
@inproceedings{gc2015breast,
  title={Breast cancer classification of mammographic masses using improved shape features},
  author={Gc, Sailesh and Pack, Chulwoo and Shin, Sung and Choi, Hyung D},
  booktitle={Proceedings of the 2015 Conference on research in adaptive and convergent systems},
  pages={188--194},
  year={2015},

  abbr={RACS}
}

@inproceedings{aminikhanghahi2015study,
  title={Study of wireless mammography image transmission impacts on robust cyber-aided diagnosis systems},
  author={Aminikhanghahi, Samaneh and Shin, Sung and Wang, Wei and Jeon, Soon I and Son, Seong H and Pack, Chulwoo},
  booktitle={Proceedings of the 30th Annual ACM Symposium on Applied Computing},
  pages={2252--2256},
  year={2015},

  abbr={SAC}
}

@inproceedings{pack2016optimized,
  title={Optimized multilayer perceptron using dynamic learning rate based microwave tomography breast cancer screening},
  author={Pack, Chulwoo and Shin, Sung and Choi, Hyung-Do and Jeon, Soon-Ik and Kim, John},
  booktitle={Proceedings of the 31st Annual ACM Symposium on Applied Computing},
  pages={2171--2175},
  year={2016},

  abbr={SAC},
  html={https://scholar.google.com/citations?view_op=view_citation&hl=en&user=P6AGgDgAAAAJ&citation_for_view=P6AGgDgAAAAJ:UeHWp8X0CEIC},
  code={https://github.com/chulwoopack/MLP_using_DLR}
}

@article{pack2022augmentation,
  title={Augmentation-based Pseudo-Ground truth Generation for Deep Learning in Historical Document Segmentation for Greater Levels of Archival Description and Access},
  author={Pack, Chulwoo and Liu, Yi and Soh, Leen-Kiat and Lorang, Elizabeth},
  journal={Journal on Computing and Cultural Heritage (JOCCH)},
  volume={15},
  number={3},
  pages={1--21},
  year={2022},
  publisher={ACM New York, NY},
  abstract = {The successful use of deep learning solutions for document image segmentation typically relies on a large number of manually labeled ground truth examples, which is expensive to obtain for historical document images that have significant noise effects and variation. At the same time, successful applications of deep learning solutions for document image segmentation have rich potential to facilitate greater levels of description in archival collections (e.g., at and below the item-level). These greater levels of description are critical to increasing access and use of archival collections across an array of research domains. In response, this article investigates whether an augmentation-based approach to generating pseudo-ground truth can be effective with a limited number of labeled images in a document segmentation application. The rationale is that if we can decrease the cost of generating ground truth through augmentation-based approaches, we can use these approaches as part of the description and access pipelines for historical library and archival collections. In this initial exploration, we first generate synthetic images and corresponding pseudo-ground truth using a set of existing degradation-based augmentation models from a small number of labeled actual images. When generating synthetic images, we control the visual quality distortion based on OCR word-level confidence to avoid generating images unlikely to be present in the dataset. Then, we perform several investigations to examine the impact of incorporating pseudo-ground truth data in the training of the deep learning network dhSegment and further evaluate the use of multiple combinations of degradation models. We also assess the generalizability of the approach by applying the trained network on a larger dataset. Our investigations primarily use real-world datasets known to have significant noise effects. Results show that augmentation-based pseudo-ground truth generation is capable of improving segmentation performance with the use of the full original dataset and requires only 30% of the original dataset. Results also show that using more than three degradation models is likely to cause overfitting during training. Furthermore, we show that a segmentation network trained on pseudo-ground truth data has generalization capability.},

  abbr={JOCCH},
  html={https://dl.acm.org/doi/abs/10.1145/3485845},
  selected={true}
}

@article{lorang2019application,
  title={Application of the Image Analysis for Archival Discovery Team’s First-Generation Methods and Software to the Burney Collection of British Newspapers},
  author={Lorang, Elizabeth and Soh, Leen-Kiat and Pack, Chulwoo and Liu, Yi and Rahimighazikalayeh, Delaram and O'Brien, John},
  year={2019},

  abbr={Tech. Report},
  html={https://digitalcommons.unl.edu/cdrhgrants/7/},
  supp={https://osf.io/u5twn/}
}

@article{pack2017optimized,
  title={Optimized Multilayer Perceptron with Dynamic Learning Rate to Classify Breast Microwave Tomography Image},
  author={Pack, Chulwoo},
  year={2017},
  publisher={South Dakota State University},

  abbr={Master}
}

@phdthesis{pack2023enhancing,
  title={Enhancing Document Layout Analysis on Historical Newspapers: Visual Representation, Pseudo-Ground-Truth, and Downscaling},
  author={Pack, Chulwoo},
  year={2023},
  school={The University of Nebraska-Lincoln},

  abbr={Ph.D.}
}

@inproceedings{zhao2022intelligent,
  title={An Intelligent Tutoring System for API Misuse Correction by Instant Quality Feedback},
  author={Zhao, Rui and Siy, Harvey and Pack, Chulwoo and Soh, Leen-Kiat and Song, Myoungkyu},
  booktitle={2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)},
  pages={123--128},
  year={2022},
  organization={IEEE},

  abbr={COMPSAC}
}

@article{pack2021visual,
  title={Visual domain knowledge-based multimodal zoning for textual region localization in noisy historical document images},
  author={Pack, Chulwoo and Soh, Leen-Kiat and Lorang, Elizabeth},
  journal={Journal of Electronic Imaging},
  volume={30},
  number={6},
  pages={063028--063028},
  year={2021},
  publisher={Society of Photo-Optical Instrumentation Engineers},
  abstract = {Document layout analysis, or zoning, is important for textual content analysis such as optical character recognition. Zoning document images such as digitized historical newspaper pages are challenging due to noise and quality of the document images. Recently, effective data-driven approaches, such as leveraging deep learning, have been proposed, albeit with the concern of requiring larger training data and thus incurring additional cost of ground truthing. We propose a zoning solution by incorporating a knowledge-driven document representation, gravity map, into a multimodal deep learning framework to reduce the amount of time and data required for training. We first generate a gravity map for each image, considering the centroid distance and area between a cell in a Voronoi tessellation and its content to encode visual domain knowledge of a zoning task. Second, we inject the gravity maps into a deep convolution neural network (DCNN) during training, as an additional modality to boost performance. We report on two investigations using two state-of-the-art DCNN architectures and three datasets: two sets of historical newspapers and a set of born-digital contemporary documents. Evaluations show that our solution achieved comparable segmentation accuracy using fewer training epochs and less training data compared to a naïve training scheme.},

  abbr={JEI},
  html={https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-30/issue-6/063028/Visual-domain-knowledge-based-multimodal-zoning-for-textual-region-localization/10.1117/1.JEI.30.6.063028.short},
  code={https://github.com/chulwoopack/gravity-map},
  selected={true}
}

@article{lorang2020final,
  title={Final Presentation to the Library of Congress on Digital Libraries, Intelligent Data Analytics, and Augmented Description},
  author={Lorang, Elizabeth and Soh, Leen-Kiat and Liu, Yi and Pack, Chulwoo},
  year={2020},

  abbr={Tech. Report},
  pdf={https://labs.loc.gov/static/labs/work/experiments/final-report-revised_june-2020.pdf},
  code={https://github.com/LibraryOfCongress/Exploring-ML-with-Project-Aida}
}

@article{lorang2019virtual,
  title={Virtual Wrap-Up Presentation: Digital Libraries, Intelligent Data Analytics, and Augmented Description},
  author={Lorang, Elizabeth and Soh, Leen-Kiat and Liu, Yi and Pack, Chulwoo},
  year={2019},

  abbr={Tech. Report}
}

@article{liu2019document,
  title={Document Images and Machine Learning: A Collaboratory between the Library of Congress and the Image Analysis for Archival Discovery (Aida) Lab at the University of Nebraska, Lincoln, NE},
  author={Liu, Yi and Pack, Chulwoo and Soh, Leen-Kiat and Lorang, Elizabeth},
  year={2019},

  abbr={Tech. Report}
}

@article{pack2019work,
  title={Work-in-Progress Reports Submitted to the Library of Congress as Part of Digital Libraries, Intelligent Data Analytics, and Augmented Description},
  author={Pack, Chulwoo and Liu, Yi and Soh, Leen-Kiat and Lorang, Elizabeth},
  year={2019},

  abbr={Tech. Report}
}

@article{pack2017computer,
  title={Computer aided diagnosis with boosted learning for anomaly detection in microwave tomography},
  author={Pack, Chulwoo and Son, Seong-Ho and Shin, Sung},
  journal={ACM SIGAPP Applied Computing Review},
  volume={17},
  number={3},
  pages={39--47},
  year={2017},
  publisher={ACM New York, NY, USA},

  abbr={ACR}
}

@article{pack2015optimized,
  title={An Optimized Fuzzy Support Vector Machine Classifier using Breast Mammogram Tomography: Trade-off between Specificity and Sensitivity},
  author={Pack, Chulwoo and Aminikhanghahi, Samaneh and Shin, Sung and Jeon, Soon Ik and Son, Seong Ho},
  journal={International Information Institute (Tokyo). Information},
  volume={18},
  number={9},
  pages={3979},
  year={2015},
  publisher={International Information Institute},

  abbr={III}
}

@article{soh2023applying,
  title={Applying Image Analysis and Machine Learning to Historical Newspaper Collections},
  author={Soh, Leen-Kiat and Lorang, Liz and Pack, Chulwoo and Liu, Yi},
  journal={The American Historical Review},
  volume={128},
  number={3},
  pages={1382--1389},
  year={2023},
  publisher={Oxford University Press},

  abbr={AHR},
  html={https://academic.oup.com/ahr/article-abstract/128/3/1382/7282221?redirectedFrom=fulltext}
}

@article{pack2024perceptual,
  title={Perceptual cue-guided adaptive image downscaling for enhanced semantic segmentation on large document images},
  author={Pack, Chulwoo and Soh, Leen-Kiat and Lorang, Elizabeth},
  journal={International Journal on Document Analysis and Recognition (IJDAR)},
  volume={27},
  number={2},
  pages={159--175},
  year={2024},
  publisher={Springer},
  abstract = {Image downscaling is an essential operation to reduce spatial complexity for various applications and is becoming increasingly important due to the growing number of solutions that rely on memory-intensive approaches, such as applying deep convolutional neural networks to semantic segmentation tasks on large images. Although conventional content-independent image downscaling can efficiently reduce complexity, it is vulnerable to losing perceptual details, which are important to preserve. Alternatively, existing content-aware downscaling severely distorts spatial structure and is not effectively applicable for segmentation tasks involving document images. In this paper, we propose a novel image downscaling approach that combines the strengths of both content-independent and content-aware strategies. The approach limits the sampling space per the content-independent strategy, adaptively relocating such sampled pixel points, and amplifying their intensities based on the local gradient and texture via the content-aware strategy. To demonstrate its effectiveness, we plug our adaptive downscaling method into a deep learning-based document image segmentation pipeline and evaluate the performance improvement. We perform the evaluation on three publicly available historical newspaper digital collections with differences in quality and quantity, comparing our method with one widely used downscaling method, Lanczos. We further demonstrate the robustness of the proposed method by using three different training scenarios: stand-alone, image-pyramid, and augmentation. The results show that training a deep convolutional neural network using images generated by the proposed method outperforms Lanczos, which relies on only content-independent strategies.},
  
  abbr={IJDAR},
  html={https://link.springer.com/article/10.1007/s10032-023-00454-7},
  code={https://github.com/chulwoopack/adaptivedownscaling},
  selected={true}
}

@inproceedings{dubey2025leveraging,
  title={Leveraging Textual Memory and Key Frame Reasoning for Full Video Understanding Using Off-the-Shelf LLMs and VLMs (Student Abstract)},
  author={Dubey, Harsh and Pack, Chulwoo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={28},
  pages={29351--29352},
  year={2025},
  abstract = {To address the limitations of current Large-scale Video-Language Models (LVLMs) in fine-grained understanding and long-term temporal memory, we propose a novel video understanding approach that integrates a Vision Language Model (VLM) and a Large Language Model (LLM) with a textual memory mechanism to ensure continuity and contextual coherence. In addition, we introduce a novel evaluation metric, VAD-Score (Video Automated Description Score), to assess precision, recall, and F1 scores for events, subjects, and objects. Our approach delivers competitive results on a diverse set of videos from the DREAM-1K dataset, spanning categories such as live-action, animation, shorts, stock, and YouTube, with a focus on fine-grained comprehension.},

  abbr={AAAI},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/35248},
  selected={true}
}

@article{10.1145/3727257.3727258,
  author = {Anigala, Omeshamisu and Won, Kwanghee and Pack, Chulwoo},
  title = {PEARL: Perceptual and Analytical Representation Learning for Video Anomaly Detection},
  year = {2025},
  issue_date = {March 2025},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {25},
  number = {1},
  issn = {1559-6915},
  url = {https://doi.org/10.1145/3727257.3727258},
  doi = {10.1145/3727257.3727258},
  abstract = {Video anomaly detection is crucial for applications like surveillance and autonomous systems. Traditional methods often rely solely on visual cues, missing valuable contextual data. This paper presents Perceptual and Analytical Representation Learning (PEARL), a novel method that combines perceptual (raw sensory input) and analytical (higher-level context) modalities. Specifically, we integrate visual information with object tracking data, along with the tracking data-specialized normalization method, DOT-Norm, leveraging ID switching to capture high-level contexts of abnormal movements. We evaluate early- and late-fusion strategies to enhance anomaly detection, particularly for irregular movements marked by frequent track ID switches. Our approach, tested on the UCSD-Ped1 dataset, outperforms the state-of-the-art by improving precision (+0.082), recall (+0.104), F1 score (+0.149), and AUC (+0.053). These findings highlight the potential of integrating analytical tracking data with perceptual video frames in a multimodal learning approach for anomaly detection, paving the way for future applications and research where knowledge-driven analytical modalities are crucial.},
  journal = {SIGAPP Appl. Comput. Rev.},
  month = apr,
  pages = {5–15},
  numpages = {11},
  keywords = {video anomaly detection, multimodal learning, object tracking},

  abbr={ACR},
  html={https://dl.acm.org/doi/10.1145/3727257.3727258},
  selected={true}
}